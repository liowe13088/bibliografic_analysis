{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3bd122",
   "metadata": {},
   "source": [
    "# Scrape Articles from Web of Science\n",
    "\n",
    "- give set of keywords\n",
    "- safe all corresponding papers as bibtex files\n",
    "\n",
    "- bibtex as zip can be used as input for biblioshiny\n",
    "\n",
    "\n",
    "For this project we will use Selenium since compared to the more basic scraping libraries it does support more functions that are needed in this case (button pushing, choosing from drop down menu). Additionally it disguises to some extend as a user and incorporates therefore some basic protection from scrape-defense mechanisms.\n",
    "\n",
    "Orientation on the following Tutorial:\n",
    "- https://thiagomarzagao.com/2013/11/12/webscraping-with-selenium-part-1/\n",
    "\n",
    "To run selenium a up to date chromedriver is needed that must be in the same folder as the skript.\n",
    "It can be found here: https://chromedriver.chromium.org/downloads\n",
    "\n",
    "Changelog v5 to v6:\n",
    "- also specifically exclude Withdrawn papers and reprints while searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "890f5536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:21:12.043833Z",
     "start_time": "2022-07-25T15:21:12.039833Z"
    }
   },
   "outputs": [],
   "source": [
    "## set up set of keywords\n",
    "\n",
    "keywords = ['portugal','lisbon','czech republic','prague','italy','rome','vatican city','hungary','budapest','romania','bucharest']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fca4f3",
   "metadata": {},
   "source": [
    "\n",
    "#### how this scraper works:\n",
    "WoS only shows the first 100.000 records of a search. no matter how many results it actually finds.\n",
    "To make sure we always get less than 100.000 records per search we search for each keyword on its own.\n",
    "Additionally, we search twice for each keyword - once for articles before 2019 (excl.) and once for articles after 2019 (incl.).\n",
    "In that fashion, each search finds less than 100.000 records and the actual scraping can be conducted without any issue.\n",
    "\n",
    "The split is not necessary for the example research presented in this repro but left in the script for exemplary reasons.\n",
    "\n",
    "1. for each keyword we generate the appropriate advanced search string\n",
    "    - innovation in all search categories\n",
    "    - the current keyword in \"TS\" (title,abstract,keywords,keyword+)\n",
    "    - the timeframe where the articles were published\n",
    "    - Doc type =  Article, not book chapter or book or proceeding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7741b6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:21:13.322263Z",
     "start_time": "2022-07-25T15:21:13.303594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate query code for advanced search:\n",
    "# depending on application this should be adapted\n",
    "\n",
    "\n",
    "def generate_query_time1(keyword):\n",
    "    query = f\"(((TS=({keyword}*)) AND DT=(Article)) NOT DT=(Book Chapter OR Proceedings Paper OR Book OR Reprint OR Withdrawn Publication)) AND PY=(1900-2018)\"\n",
    "    return query\n",
    "\n",
    "def generate_query_time2(keyword):\n",
    "    query = f\"(((TS=({keyword}*)) AND DT=(Article)) NOT DT=(Book Chapter OR Proceedings Paper OR Book OR Reprint OR Withdrawn Publication)) AND PY=(2019-2021)\"\n",
    "    return query\n",
    "\n",
    "\n",
    "queries = [generate_query_time1(kw) for kw in keywords]+[generate_query_time2(kw) for kw in keywords]\n",
    "print(len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa6c334b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:21:14.648885Z",
     "start_time": "2022-07-25T15:21:14.635387Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "\n",
    "# define actual scraping function\n",
    "\n",
    "# get number of iterations: n_papers/1000\n",
    "\n",
    "def scrape_search(n_papers,keyword,batch):\n",
    "    '''\n",
    "    input: n_papers - number of papers that the current search found\n",
    "        keyword - keyword on which current search is based\n",
    "        batch - 0 or 1 depending on before or after 2019\n",
    "        \n",
    "    output: none, saves BibTeX files directly under an appropriate name (indicating keyword and batch)\n",
    "    \n",
    "    the function is downloading all records of a search. thereby each BibTeX file can contain a maximum of 1000 records\n",
    "    if a search contains more records the function detects that automatically and downloads all records, saving them \n",
    "    in n_papers/1000 papers.\n",
    "    '''\n",
    "\n",
    "    iterations = math.ceil(n_papers/1000)\n",
    "    start = 1\n",
    "    end = 1000\n",
    "\n",
    "    # for each iteration:\n",
    "    for i in tqdm(range(iterations)):\n",
    "                \n",
    "        time.sleep(5)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # 1. click Export button to open drop down\n",
    "                browser.find_element(By.XPATH, '//*[@id=\"snRecListTop\"]/app-export-menu/div/button').click()\n",
    "                break\n",
    "            except:\n",
    "                time.sleep(2)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        # 2. click on BibTex\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"exportToBibtexButton\"]').click()\n",
    "        \n",
    "\n",
    "        time.sleep(1)\n",
    "        # 3. open \"Record Content\" drop down menu and choose \"Full Record\"\n",
    "        browser.find_element(By.XPATH, '/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/div[1]/wos-select/button').click()\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"global-select\"]/div/div[2]/div[3]').click()\n",
    "\n",
    "        time.sleep(1)\n",
    "        # 4. choose \"Records from\" and enter correct number depending on iteration\n",
    "        browser.find_element(By.XPATH, '//*[@id=\"radio3\"]/label/span[1]').click()\n",
    "        time.sleep(1)\n",
    "        browser.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/fieldset/mat-radio-group/div[3]/mat-form-field[1]/div/div[1]/div[3]/input').clear()\n",
    "        browser.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/fieldset/mat-radio-group/div[3]/mat-form-field[1]/div/div[1]/div[3]/input').send_keys(start)\n",
    "        browser.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/fieldset/mat-radio-group/div[3]/mat-form-field[2]/div/div[1]/div[3]/input').clear()\n",
    "        browser.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/fieldset/mat-radio-group/div[3]/mat-form-field[2]/div/div[1]/div[3]/input').send_keys(end)\n",
    "\n",
    "        # update start and end\n",
    "        start += 1000\n",
    "        end += 1000\n",
    "\n",
    "        # 5. click export and save resulting bibtex file\n",
    "        browser.find_element(By.XPATH, '/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route[1]/app-export-overlay/div/div[3]/div[2]/app-export-out-details/div/div[2]/form/div/div[2]/button[1]').click()\n",
    "        \n",
    "        ## problems with the naming\n",
    "        ## wait until download ended before renaming\n",
    "        file_path = r'C:\\Users\\Lion\\Documents\\UNI4_RWTH\\tim_hiwi_job\\bibliometric_analysis\\github_public\\scrape_data\\savedrecs.bib'\n",
    "        \n",
    "        while not os.path.exists(file_path):\n",
    "            time.sleep(1)\n",
    "\n",
    "        if os.path.isfile(file_path):\n",
    "            filename = r\"scrape_data/savedrecs.bib\"\n",
    "            new_filename = f\"scrape_data/{keyword}_{i}{batch}.bib\"\n",
    "            os.rename(filename,new_filename)\n",
    "        else:\n",
    "            raise ValueError(\"%s isn't a file!\" % file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0918ed50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:21:29.526411Z",
     "start_time": "2022-07-25T15:21:29.516979Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# define function to extract the keyword from the generated query\n",
    "# to pass that keyword to our scrape function so we can name the bibtex files correctly\n",
    "\n",
    "def extract_kw(q):\n",
    "    m = re.search(\"[a-z]+([_&]?[a-z]+)*\",q)\n",
    "    return m.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75caabaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:21:32.108745Z",
     "start_time": "2022-07-25T15:21:30.929413Z"
    }
   },
   "outputs": [],
   "source": [
    "## starts a new chrome window, dont close!\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# download chromedriver here:\n",
    "# https://sites.google.com/chromium.org/driver/\n",
    "# unpack zip and add to skript directory, change path if needed\n",
    "\n",
    "# set download directory\n",
    "prefs = {'download.default_directory' : r'C:\\Users\\Lion\\Documents\\UNI4_RWTH\\tim_hiwi_job\\bibliometric_analysis\\github_public\\scrape_data'}\n",
    "options = Options()\n",
    "options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "# # start instance of browser with selenium\n",
    "path_to_chromedriver = 'chromedriver' # change path as needed\n",
    "browser = webdriver.Chrome(executable_path = path_to_chromedriver,\n",
    "                          options = options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d30bdb36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T17:06:41.629158Z",
     "start_time": "2022-07-25T15:31:19.907430Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portugal\n",
      "0: portugal: 25226 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [08:14<00:00, 19.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lisbon\n",
      "1: lisbon: 3824 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [01:13<00:00, 18.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "czech\n",
      "2: czech: 16960 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 17/17 [05:17<00:00, 18.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prague\n",
      "3: prague: 4415 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:27<00:00, 17.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy\n",
      "4: italy: 96319 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 97/97 [31:15<00:00, 19.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rome\n",
      "5: rome: 17057 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [05:31<00:00, 18.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vatican\n",
      "6: vatican: 85 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungary\n",
      "7: hungary: 18059 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [05:52<00:00, 18.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "budapest\n",
      "8: budapest: 2491 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:52<00:00, 17.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romania\n",
      "9: romania: 19478 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [06:13<00:00, 18.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucharest\n",
      "10: bucharest: 1590 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:36<00:00, 18.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portugal\n",
      "11: portugal: 9377 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [03:15<00:00, 19.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lisbon\n",
      "12: lisbon: 1444 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:37<00:00, 18.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "czech\n",
      "13: czech: 5299 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:52<00:00, 18.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prague\n",
      "14: prague: 1075 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:33<00:00, 16.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy\n",
      "15: italy: 34376 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [11:54<00:00, 20.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rome\n",
      "16: rome: 5295 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:48<00:00, 18.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vatican\n",
      "17: vatican: 48 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hungary\n",
      "18: hungary: 5256 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:53<00:00, 18.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "budapest\n",
      "19: budapest: 663 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romania\n",
      "20: romania: 8341 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [02:53<00:00, 19.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucharest\n",
      "21: bucharest: 747 papers found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:17<00:00, 17.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# uses the open chrome window to do search for each query and call scrape function\n",
    "\n",
    "\n",
    "url = \"https://www.webofscience.com/wos/woscc/advanced-search\"\n",
    "\n",
    "# if scraping interrupts (often on WoS side with \"internal server error)\n",
    "# put in here - if first batch done -  only once, if both twice\n",
    "done = []\n",
    "\n",
    "# loop that searches each generated query and then calls our earlier defined scraping function\n",
    "\n",
    "for count,query in enumerate(queries): #keep track of number of iterations to define batch (count) with enumerate()\n",
    "    \n",
    "    # generate batch info for file naming\n",
    "    # enter manually based on on queries\n",
    "    if count <= 10:\n",
    "        batch=1\n",
    "    else: batch=2\n",
    "    \n",
    "    # extract the current keyword from the query\n",
    "    kw = extract_kw(query)\n",
    "    \n",
    "    print(kw)\n",
    "    if kw in done:\n",
    "        done.remove(kw)\n",
    "    else:\n",
    "        # open page and conduct search\n",
    "        browser.get(url)\n",
    "\n",
    "        # important to cancel out the time needed to load certain element on a web page\n",
    "        # sleep statements might have to be longer or can be shorter depending on the internet connection\n",
    "        time.sleep(2)\n",
    "\n",
    "        # reject cookies if needed (only first time)\n",
    "        try:\n",
    "            browser.find_element(By.XPATH,'//*[@id=\"onetrust-reject-all-handler\"]').click()\n",
    "        except: pass\n",
    "\n",
    "        # put generated query into search form after clearing searh form\n",
    "        browser.find_element(By.ID,'advancedSearchInputArea').clear()\n",
    "        browser.find_element(By.ID,'advancedSearchInputArea').send_keys(query)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # click search button\n",
    "        browser.find_element(By.XPATH,'/html/body/app-wos/div/div/main/div/div/div[2]/app-input-route/app-search-home/div[2]/div/app-input-route/app-search-advanced/app-advanced-search-form/form/div[3]/div[1]/div[1]/div/button[2]').click()\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "        # get and print number of papers\n",
    "\n",
    "        n_paper = int(browser.find_element(By.CLASS_NAME, \"brand-blue\").text.replace(\",\",\"\"))\n",
    "        print(f\"{count}: {kw}: {n_paper} papers found\")\n",
    "\n",
    "        # close helper window on webpage if needed\n",
    "        try:\n",
    "            browser.find_element(By.XPATH, '//*[@id=\"pendo-button-e5808a4c\"]').click()\n",
    "        except: pass\n",
    "\n",
    "        # call scrape function\n",
    "        scrape_search(n_paper,kw,batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdcd987",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
